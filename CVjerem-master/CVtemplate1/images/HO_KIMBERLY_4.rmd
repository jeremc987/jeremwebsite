---
title: "Week 4"
subtitle: "Discriminant Analysis"
author: HO Kimberly
date: "30/01/2017"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: cerulean
    highlight: espresso
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This week we are going to continue the analysis of the Social_Network_Ads dataset. Recall that this dataset contains informations of users of a social network and if they bought a specified product. Last week we built a Logistic Regression model for the variable Purchased in function of Age and EstimatedSalary. We will consider the same variables this week but we will fit different models using methods such as LDA, QDA, and Naive Bayes.

#Logistic Regression
##Exercise 1
First, let’s do the pre-processing steps you were asked to do during the last session and fit a logistic regression model. Please read and understand very well the following code (read the comments!). Then copy what is necessary for today’s session to your report (but remove my comments!).

```{r}
Social_Network_Ads=read.csv("Social_Network_Ads.csv")
dataset = Social_Network_Ads[,-c(1:2)]
dataset


library(caTools)
set.seed(345679) # CHANGE THE VALUE OF SEED. PUT YOUR STUDENT'S NUMBER INSTEAD OF 123.
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)


training_set[-3] <- scale(training_set[-3]) #only first two columns
test_set[-3] <- scale(test_set[-3])


classifier.logreg <- glm(Purchased ~ Age + EstimatedSalary , family = binomial, data=training_set)
classifier.logreg
summary(classifier.logreg)


```

#Decision Boundary of Logistic Regression
##Exercise 2
Plot the decision boundary obtained with logistic regression. In order to do so, calculate the intercept and the slope of the line presenting the decision boundary, then plot EstimatedSalary in function of Age (from the test_set) and add the line using abline().

```{r}
# Decision boundary equation
beta0 = classifier.logreg$coefficients[1]
beta1 = classifier.logreg$coefficients[2]
beta2 = classifier.logreg$coefficients[3]

slope = -(beta1/beta2)
intercept = -(beta0/beta2)

#X2 = -(beta1/beta2)*X1-(beta0/beta2)

plot(x=test_set$Age , y=test_set$EstimatedSalary,xlab="Age", ylab="Estimated Salary",main="Estimated Salary in function of Age", pch=20, col="red" )
abline(intercept,slope, col="blue")
```


##Exercise 3
In order to verify that your line (decision boundary) is well plotted, color the points on the last Figure with respect to the predicted response.

Hints:

If your predictions are stored in y_pred, you can do it using bg = ifelse(y_pred == 1, 'color1', 'color2'), and precise the argument pch to be 21 (you can choose pch to be a value between 21 and 25, try it).
Then, add the line using abline(), put the line width = 2 to make it more visible. Do not forget to title the Figure).

```{r}
Y = beta0 + beta1*test_set$Age + beta2*test_set$EstimatedSalary


plot(x=test_set$Age , y=test_set$EstimatedSalary,xlab="Age", ylab="Estimated Salary",main="Estimated Salary in function of Age", pch=24,bg=ifelse( Y > 0,"red", "black"))
abline(intercept,slope, col="blue",lwd=2)
```


##Exercise 4
Now make the same plot but color the points with respect to their real labels (the variable Purchased). From this figure, count the number of the false positive predictions and compare it to the value obtained in the confusion matrix.

```{r}
plot(x=test_set$Age , y=test_set$EstimatedSalary,xlab="Age", ylab="Estimated Salary",main="Estimated Salary in function of Age", pch=24,bg=ifelse( Y > 0,"red", "black"))
abline(intercept,slope, col="blue",lwd=2)
# color the plotted points with their real label (class)
points(test_set, pch = 22, bg = ifelse(test_set[, 3] == 1, 'green4', 'red3'))

y_pred = predict(classifier.logreg,test_set,type="response")
obs = ifelse(y_pred>0.5,1,0)

confusion_matrix=table(test_set$Purchased,obs)
confusion_matrix
```


#Linear Discriminant Analysis (LDA)
Let us apply linear discriminant analysis (LDA) now. First we will make use of the lda() function in the package MASS. Second, you are going to create the model and predict the classes by yourself without using the lda() function. And we will visualize the decision boundary of LDA.
##Exercise 5
Fit a LDA model of Purchased in function of Age and EstimatedSalary. Name the model classifier.lda.
```{r}
library(MASS)
classifier.lda = lda(Purchased~Age+EstimatedSalary, data=training_set)
summary(classifier.lda)
```


##Exercise 6
Call classifier.lda and see what does it compute.

Plus: If you enter the following you will be returned with a list of summary information concerning the computation:
```{r}
classifier.lda
classifier.lda$prior
classifier.lda$means
```


##Exercise 7
On the test set, predict the probability of purchasing the product by the users using the model classifier.lda. Remark that when we predict using LDA, we obtain a list instead of a matrix, do str() for your predictions to see what do you get.
Remark: we get the predicted class here, without being obligated to round the predictions as we did for logistic regression.

```{r}
y_pred_lda = predict(classifier.lda,test_set,type="response")
str(y_pred_lda)

```

##Exercise 8
Compute the confusion matrix and compare the predictions results obtained by LDA to the ones obtained by logistic regression. What do you remark?

(Hint: compare the accuracy)
```{r}
confusion_matrix_lda = table(test_set$Purchased,y_pred_lda$class)
confusion_matrix_lda

TPos = confusion_matrix[2,2]
TNeg = confusion_matrix[1,1]
FPos = confusion_matrix[2,1]
FNeg = confusion_matrix[1,2]

accuracy = (TPos + TNeg) / (TPos + TNeg + FPos + FNeg)
accuracy

TPos_lda = confusion_matrix_lda[2,2]
TNeg_lda = confusion_matrix_lda[1,1]
FPos_lda = confusion_matrix_lda[2,1]
FNeg_lda = confusion_matrix_lda[1,2]

accuracy_lda = (TPos_lda + TNeg_lda) / (TPos_lda + TNeg_lda + FPos_lda + FNeg_lda)
accuracy_lda
```
We remark that we have approximately the same confusion matrix and accuracy for the LDA and the logistic regression

##Exercise 9
Now let us plot the decision boundary obtained with LDA. You saw in the course that decision boundary for LDA represent the set of values x where ??k(x)=??c(x). Recall that ??k(X)=xT?????1??k???12??Tk?????1??k+log??k

Here in our case, we have 2 classes ( K=2 ) and 2 predictors ( p=2 ). So the decision boundary (which is linear in the case of LDA, and line in our case since p=2 ) will verify the equation ??0(x)=??1(x) Since we have two classes "0" and "1". In the case of LDA this leads to linear boundary and is easy to be plotted. But in more complicated cases it is difficult to manually simplify the equations and plot the decision boundary. Anyway, there is a smart method to plot (but a little bit costy) the decision boundary in R using the function contour(), the corresponding code is the following (you must adapt it and use it to plot your decision boundary):

```{r}
# create a grid corresponding to the scales of Age and EstimatedSalary
# and fill this grid with lot of points
X1 = seq(min(training_set[, 1]) - 1, max(training_set[, 1]) + 1, by = 0.01)
X2 = seq(min(training_set[, 2]) - 1, max(training_set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
# Adapt the variable names
colnames(grid_set) = c("Age", "EstimatedSalary")
# plot 'Estimated Salary' ~ 'Age'
plot(test_set[, -3], main = 'Decision Boundary LDA', xlab = 'Age', ylab = 'Estimated Salary', xlim = range(X1), ylim = range(X2))
# color the plotted points with their real label (class)
points(test_set, pch = 21, bg = ifelse(test_set[, 3] == 1, 'green4', 'red3'))
# Make predictions on the points of the grid, this will take some time
classifier.lda = lda(Purchased~Age+EstimatedSalary, data=training_set)
pred_grid = predict(classifier.lda,newdata=grid_set)
pred_grid = pred_grid$class
# Separate the predictions by a contour
contour(X1, X2, matrix(as.numeric(pred_grid), length(X1), length(X2)), add = TRUE)

```


##Exercise 10
Now let us build a LDA model for our data set without using the lda() function. You are free to do it by creating a function or without creating one. Go back to question 6 and see what did you obtain by using lda(). It computes the prior probability of group membership and the estimated group means for each of the two groups. Additional information that is not provided, but may be important, is the single covariance matrix that is being used for the various groupings.
So let us do it step by step, first we will do the estimates:
###Exercise 10.1
Subset the training set into two sets: class0 where Purchased = 0 and class1 where Purchased = 1).
```{r}

class0 = subset(training_set,Purchased==0,select = c(Age, EstimatedSalary, Purchased))
class1 = subset(training_set,Purchased==1,select = c(Age, EstimatedSalary, Purchased))

class0
class1
```


###Exercise 10.2
Compute π0 and π1. πi=Ni/N,where Ni is the number of data points in group i
```{r}
N0 = dim(class0)[1]
N1 = dim(class1)[1]
pi0 = N0/(N0+N1)
pi1 = N1/(N0+N1)
pi0
pi1

```


###Exercise 10.3
Compute μ0 and μ1. μ0=(μ0(X1)μ0(X2)) and μ1=(μ1(X1)μ1(X2)) where, for example, μ0(X1) is the mean of the variable X1 in the group 0 (the subset class0).

```{r}
u0 = c(mean(class0$Age),mean(class0$EstimatedSalary))
u1 = c(mean(class1$Age),mean(class1$EstimatedSalary))

u0
u1
```


###Exercise 10.4
Compute  Σ . In the case of two classes like here, it is computed by calculating the following: Σ=(N0-1)Σ0+(N1-1)Σ1/N0+N1-2 where Σi is the estimated covariance matrix for specific group i . 

Remark: Recall that in LDA we use the same  Σ . But in QDA we do not.

```{r}
sigma0 = cov(class0$Age,class0$EstimatedSalary)
sigma1 = cov(class1$Age,class1$EstimatedSalary)

sigma = ((N0-1)*sigma0+(N1-1)*sigma1)/(N0+N1-2)
sigma

```


###Exercise 10.5
Now that we have computed all the needed estimates, we can calculate δ0(x) and δ1(x) for any observation x . And we will attribute x to the class with the highest δ. First, try it for x where xT=(1,1.5), what is class prediction for this specific x ?

```{r}
xT = c(1,1.5)
sigma = as.vector(sigma)
delta0 = xT*sigma^(-1)*u0 - (1/2)*t(u0)*sigma^(-1)*u0+log(pi0)
delta0

delta1 = xT*sigma^(-1)*u1 - (1/2)*t(u1)*sigma^(-1)*u1+log(pi1)
delta1


class(delta0)
class(delta1)
```


###Exercise 10.6
Compute the discriminant scores δ for the test set (a matrix 100 X 2 ), predict the classes and compare your results with the results obtained with the lda() function.
```{r}
xT = test_set[-3]
xT
delta0 = xT*sigma^(-1)*u0 - (1/2)*t(u0)*sigma^(-1)*u0+log(pi0)
delta0

delta1 = xT*sigma^(-1)*u1 - (1/2)*t(u1)*sigma^(-1)*u1+log(pi1)
delta1


class(delta0)
class(delta1)
```

#Quadratic Discriminant Analysis (QDA)
Training and assessing a QDA model in R is very similar in syntax to training and assessing a LDA model. The only difference is in the function name qda()
##Exercise 11
Fit a QDA model of Purchased in function of Age and EstimatedSalary. Name the model classifier.qda.
```{r}
classifier.qda = qda(Purchased~., data = training_set)
summary(classifier.qda)
```


##Exercise 12
Make predictions on the test_set using the QDA model classifier.qda. Show the computation matrix and compare the results with the predictions obtained using the LDA model classifier.lda.
```{r}
y_pred_qda = predict(classifier.qda,test_set,type="response")
#str(y_pred_qda)

confusion_matrix_qda = table(test_set$Purchased,y_pred_qda$class)
confusion_matrix_qda


TPos_qda = confusion_matrix_qda[2,2]
TNeg_qda = confusion_matrix_qda[1,1]
FPos_qda = confusion_matrix_qda[2,1]
FNeg_qda = confusion_matrix_qda[1,2]

accuracy_qda = (TPos_qda + TNeg_qda) / (TPos_qda + TNeg_qda + FPos_qda + FNeg_qda)
accuracy_qda

confusion_matrix_lda
```
Comparing both of the computation matrix, we can see a small difference. For the LDA, TP = 57, TN = 26, FP = 7, FN = 10, and for the QDA, TP = 58, TN = 32, FP = 6, FN = 4.  

##Exercise 13
Plot the decision boundary obtained with QDA. Color the points with the real labels.

```{r}
# create a grid corresponding to the scales of Age and EstimatedSalary
# and fill this grid with lot of points
X1 = seq(min(training_set[, 1]) - 1, max(training_set[, 1]) + 1, by = 0.01)
X2 = seq(min(training_set[, 2]) - 1, max(training_set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
# Adapt the variable names
colnames(grid_set) = c("Age", "EstimatedSalary")
# plot 'Estimated Salary' ~ 'Age'
plot(test_set[, -3], main = 'Decision Boundary QDA', xlab = 'Age', ylab = 'Estimated Salary', xlim = range(X1), ylim = range(X2))
# color the plotted points with their real label (class)
points(test_set, pch = 21, bg = ifelse(test_set[, 3] == 1, 'green4', 'red3'))
# Make predictions on the points of the grid, this will take some time
classifier.qda = qda(Purchased~Age+EstimatedSalary, data=training_set)
pred_grid = predict(classifier.qda,newdata=grid_set)
pred_grid = pred_grid$class
# Separate the predictions by a contour
contour(X1, X2, matrix(as.numeric(pred_grid), length(X1), length(X2)), add = TRUE)
```


#Comparison
##Exercise 14
In order to compare the methods we used, plot on the same Figure the ROC curve for each classifier we fitted and compare the correspondant AUC. What was the best model for this dataset?

Remark: If you use the ROCR package:

For Logistic regression, use the predicted probabilities in the prediction() (and not the round values "0" or "1").
For LDA and QDA, put pred.lda$posterior[,2] in the prediction() function (those are the posterior probabilities that observations belong to class "1").
```{r}
library(ROCR)
y_pred = predict(classifier.logreg,test_set,type='response')
pred = prediction(y_pred,test_set$Purchased)
plot(performance(pred, measure = "tpr", x.measure = "fpr"),col="blue", main="ROC LR")
abline(a=0, b= 1,col="yellow",lwd=2)
obs = ifelse(y_pred>0.5,1,0)


y_pred_lda = predict(classifier.lda,test_set,type='response')
pred_lda = prediction(y_pred_lda$posterior[,2],test_set$Purchased)
plot(performance(pred_lda, measure = "tpr", x.measure = "fpr"),col="black", main="ROC LDA")
abline(a=0, b= 1,col="yellow",lwd=2)


y_pred_qda = predict(classifier.qda,test_set,type='response')
pred_qda = prediction(y_pred_qda$posterior[,2],test_set$Purchased)
plot(performance(pred_qda, measure = "tpr", x.measure = "fpr"),col="red", main="ROC QDA")
abline(a=0, b= 1,col="yellow",lwd=2)


AUC = performance(prediction(obs,test_set$Purchased),"auc")
AUC
AUC_lda = performance(prediction(y_pred_lda$posterior[,2]  ,test_set$Purchased),"auc")
AUC_lda
AUC_qda = performance(prediction(y_pred_qda$posterior[,2],test_set$Purchased),"auc")
AUC_qda

```
For the AUC: AUC (logistic regression) = 0,78 < AUC (LDA) = 0,90 < AUC (QDA) = 0,92. So the best model is the QDA.

